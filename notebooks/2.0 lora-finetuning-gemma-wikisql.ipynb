{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11384,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":6216}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/peft.git ","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:33:00.374607Z","iopub.execute_input":"2024-02-28T00:33:00.375367Z","iopub.status.idle":"2024-02-28T00:33:44.181529Z","shell.execute_reply.started":"2024-02-28T00:33:00.375318Z","shell.execute_reply":"2024-02-28T00:33:44.180226Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"pip install -U transformers==4.38.1 -q","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:33:44.183644Z","iopub.execute_input":"2024-02-28T00:33:44.183949Z","iopub.status.idle":"2024-02-28T00:34:07.419039Z","shell.execute_reply.started":"2024-02-28T00:33:44.183919Z","shell.execute_reply":"2024-02-28T00:34:07.417858Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\nimport os\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:34:07.420698Z","iopub.execute_input":"2024-02-28T00:34:07.421046Z","iopub.status.idle":"2024-02-28T00:34:09.022666Z","shell.execute_reply.started":"2024-02-28T00:34:07.421016Z","shell.execute_reply":"2024-02-28T00:34:09.021849Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# incase dataset loading raises an error\n# ! pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:24:41.747064Z","iopub.execute_input":"2024-02-28T00:24:41.747512Z","iopub.status.idle":"2024-02-28T00:24:41.751301Z","shell.execute_reply.started":"2024-02-28T00:24:41.747485Z","shell.execute_reply":"2024-02-28T00:24:41.750288Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"Dataset_id = 'wikisql'\ndataset = load_dataset(Dataset_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:35:16.191015Z","iopub.execute_input":"2024-02-28T00:35:16.191955Z","iopub.status.idle":"2024-02-28T00:36:02.411055Z","shell.execute_reply.started":"2024-02-28T00:35:16.191918Z","shell.execute_reply":"2024-02-28T00:36:02.410147Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb06f4f7f0447a691083265a37c8a16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/875 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6298b59c0c34609b7a96b82ffc1858c"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset wiki_sql/default (download: 24.95 MiB, generated: 147.57 MiB, post-processed: Unknown size, total: 172.52 MiB) to /root/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/26.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a6250ef85a45edaba7b73bbe72cb75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/15878 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/8421 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/56355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset wiki_sql downloaded and prepared to /root/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802158511c1a4386ac325a463d8a1db8"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom datasets.dataset_dict import DatasetDict\nimport pandas as pd\nclass SQLDataSet(Dataset):\n    def __init__(self,raw_dataset:DatasetDict,split='train'):\n        self._initialize(raw_dataset,split)\n    \n    def _initialize(self,raw_dataset,split):\n        subset = raw_dataset[split]\n        answers = [ans.get('human_readable') for ans in subset['sql']]\n        self.dataframe = pd.DataFrame({'question':subset['question'],\n                                      'answer':answers})\n    def __len__(self):\n        return self.dataframe.shape[0]\n    \n    def __getitem__(self,index):\n        return self.dataframe.iloc[index,:].to_dict()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:36:02.413027Z","iopub.execute_input":"2024-02-28T00:36:02.413867Z","iopub.status.idle":"2024-02-28T00:36:02.422778Z","shell.execute_reply.started":"2024-02-28T00:36:02.413830Z","shell.execute_reply":"2024-02-28T00:36:02.421931Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = SQLDataSet(dataset,split='train')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:36:02.423926Z","iopub.execute_input":"2024-02-28T00:36:02.424249Z","iopub.status.idle":"2024-02-28T00:36:04.532146Z","shell.execute_reply.started":"2024-02-28T00:36:02.424223Z","shell.execute_reply":"2024-02-28T00:36:04.531392Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\nfor i,sample in enumerate(train):\n    display(Markdown(f\"### Question {i+1}:\\n{sample['question']}\\n### Answer:\\n{sample['answer']}\"))\n    if i==3:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:36:04.534771Z","iopub.execute_input":"2024-02-28T00:36:04.535583Z","iopub.status.idle":"2024-02-28T00:36:04.550733Z","shell.execute_reply.started":"2024-02-28T00:36:04.535545Z","shell.execute_reply":"2024-02-28T00:36:04.549730Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 1:\nTell me what the notes are for South Australia \n### Answer:\nSELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 2:\nWhat is the current series where the new series began in June 2011?\n### Answer:\nSELECT Current series FROM table WHERE Notes = New series began in June 2011"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 3:\nWhat is the format for South Australia?\n### Answer:\nSELECT Format FROM table WHERE State/territory = South Australia"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 4:\nName the background colour for the Australian Capital Territory\n### Answer:\nSELECT Text/background colour FROM table WHERE State/territory = Australian Capital Territory"},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:36:04.551892Z","iopub.execute_input":"2024-02-28T00:36:04.552293Z","iopub.status.idle":"2024-02-28T00:36:04.667936Z","shell.execute_reply.started":"2024-02-28T00:36:04.552255Z","shell.execute_reply":"2024-02-28T00:36:04.667073Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['phase', 'question', 'table', 'sql'],\n        num_rows: 15878\n    })\n    validation: Dataset({\n        features: ['phase', 'question', 'table', 'sql'],\n        num_rows: 8421\n    })\n    train: Dataset({\n        features: ['phase', 'question', 'table', 'sql'],\n        num_rows: 56355\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"val = SQLDataSet(dataset,'validation')\ntest = SQLDataSet(dataset,'test')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:36:04.669392Z","iopub.execute_input":"2024-02-28T00:36:04.669844Z","iopub.status.idle":"2024-02-28T00:36:05.633756Z","shell.execute_reply.started":"2024-02-28T00:36:04.669769Z","shell.execute_reply":"2024-02-28T00:36:05.632979Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Load Base Model","metadata":{}},{"cell_type":"code","source":"!pip install --quiet --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:36:05.634884Z","iopub.execute_input":"2024-02-28T00:36:05.635190Z","iopub.status.idle":"2024-02-28T00:36:19.004021Z","shell.execute_reply.started":"2024-02-28T00:36:05.635163Z","shell.execute_reply":"2024-02-28T00:36:19.002942Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Setup the environment\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\naccess_token_read = UserSecretsClient().get_secret(\"HF\")\nlogin(token = access_token_read)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:36:19.005499Z","iopub.execute_input":"2024-02-28T00:36:19.005799Z","iopub.status.idle":"2024-02-28T00:36:19.235069Z","shell.execute_reply.started":"2024-02-28T00:36:19.005770Z","shell.execute_reply":"2024-02-28T00:36:19.234227Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# pip install -U flash-attn==2.5.5 --no-build-isolation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nMODEL_DIR = \"/kaggle/input/gemma/transformers/2b/2\"\n# for fune-tunning it's best practise to load the model in 16 bit it's give the best \nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\n                                        load_4bit_use_double_quant=True,\n                                        bnb_4bit_quant_type=\"nf4\",\n                                        bnb_4bit_compute_dtype=torch.bfloat16)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR,\n                                        add_eos_token=True,\n                                         )\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR,\n                                             quantization_config=quantization_config,\n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float32,\n                                             trust_remote_code=True,\n#                                             attn_implementation=\"flash_attention_2\") # needs only special GPUs\n                                            )","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:42:08.274391Z","iopub.execute_input":"2024-02-28T00:42:08.275307Z","iopub.status.idle":"2024-02-28T00:42:18.018073Z","shell.execute_reply.started":"2024-02-28T00:42:08.275266Z","shell.execute_reply":"2024-02-28T00:42:18.017134Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c870b5b0804acaa24f070ca83f75a8"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:42:22.785020Z","iopub.execute_input":"2024-02-28T00:42:22.786145Z","iopub.status.idle":"2024-02-28T00:42:22.793967Z","shell.execute_reply.started":"2024-02-28T00:42:22.786110Z","shell.execute_reply":"2024-02-28T00:42:22.792998Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"target_modules = ['q_proj',\n                 'k_proj',\n                 'v_proj',\n                 'o_proj',\n#                  'gate_proj',\n#                  'up_proj',\n#                  'down_proj',\n#                  'lora_magnitude_vector'\n                 ]\nunfreeze= [ 'embed_tokens',\n          'input_layernorm',\n          'post_attention_layernorm']","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:45:32.134559Z","iopub.execute_input":"2024-02-28T00:45:32.135206Z","iopub.status.idle":"2024-02-28T00:45:32.140018Z","shell.execute_reply.started":"2024-02-28T00:45:32.135173Z","shell.execute_reply":"2024-02-28T00:45:32.138982Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Loading Check ","metadata":{}},{"cell_type":"code","source":"# check that all paramters are existing in GPU not in cpu(meta)\nfor n,p in model.named_parameters():\n    if p.device.type == 'meta':\n        print(f\"{n} is on meta\")","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:42:33.780963Z","iopub.execute_input":"2024-02-28T00:42:33.781447Z","iopub.status.idle":"2024-02-28T00:42:33.787930Z","shell.execute_reply.started":"2024-02-28T00:42:33.781408Z","shell.execute_reply":"2024-02-28T00:42:33.786933Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Test before Fine-tuning","metadata":{}},{"cell_type":"code","source":"def generate(question:str,model):\n    input_ids = tokenizer(question,\n                          padding=True,\n                          truncation=True,\n                          return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**input_ids,\n                            max_new_tokens=50,\n                            do_sample=True,\n                            top_p=0.92,\n                            top_k=0,\n)\n    return tokenizer.decode(outputs[0],skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:42:36.362858Z","iopub.execute_input":"2024-02-28T00:42:36.363541Z","iopub.status.idle":"2024-02-28T00:42:36.369458Z","shell.execute_reply.started":"2024-02-28T00:42:36.363504Z","shell.execute_reply":"2024-02-28T00:42:36.368334Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:42:36.687174Z","iopub.execute_input":"2024-02-28T00:42:36.687497Z","iopub.status.idle":"2024-02-28T00:42:36.695430Z","shell.execute_reply.started":"2024-02-28T00:42:36.687471Z","shell.execute_reply":"2024-02-28T00:42:36.694512Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"GemmaConfig {\n  \"_name_or_path\": \"/kaggle/input/gemma/transformers/2b/2\",\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 18,\n  \"num_key_value_heads\": 1,\n  \"pad_token_id\": 0,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": false,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.38.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 256000\n}"},"metadata":{}}]},{"cell_type":"code","source":"test_data = ['What is the capital of Italy?',\n            'What are the top 3 highest grossing movies of all time?',\n            'Write me a poem about Machine Learning.',\n            'what is ']\nfor q in test_data:\n    print(generate(q,model))","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:26:50.419061Z","iopub.execute_input":"2024-02-28T00:26:50.419444Z","iopub.status.idle":"2024-02-28T00:26:58.246891Z","shell.execute_reply.started":"2024-02-28T00:26:50.419419Z","shell.execute_reply":"2024-02-28T00:26:58.245985Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n2024-02-28 00:26:52.545642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-28 00:26:52.545698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-28 00:26:52.547203: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"What is the capital of Italy?\nWhat are the top 3 highest grossing movies of all time?\nWrite me a poem about Machine Learning.\nwhat is \n","output_type":"stream"}]},{"cell_type":"code","source":"for i,sample in enumerate(train):\n    generated_ans = generate(sample['question']+\"?\",model)\n    display(Markdown(f\"### Question {i+1}:\\n{sample['question']}\\n### Generated Answer:\\n{generated_ans}\\n### Actual Answer:\\n{sample['answer']}\"))\n    if i==3:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:26:59.772170Z","iopub.execute_input":"2024-02-28T00:26:59.772890Z","iopub.status.idle":"2024-02-28T00:27:59.134516Z","shell.execute_reply.started":"2024-02-28T00:26:59.772859Z","shell.execute_reply":"2024-02-28T00:27:59.133549Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 1:\nTell me what the notes are for South Australia \n### Generated Answer:\nTell me what the notes are for South Australia ?\n### Actual Answer:\nSELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 2:\nWhat is the current series where the new series began in June 2011?\n### Generated Answer:\nWhat is the current series where the new series began in June 2011??\n### Actual Answer:\nSELECT Current series FROM table WHERE Notes = New series began in June 2011"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 3:\nWhat is the format for South Australia?\n### Generated Answer:\nWhat is the format for South Australia??\n### Actual Answer:\nSELECT Format FROM table WHERE State/territory = South Australia"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Question 4:\nName the background colour for the Australian Capital Territory\n### Generated Answer:\nName the background colour for the Australian Capital Territory?\n\nSpain, also known as Spania, is one of the 19 countries of the European Union and the European Mainland.\n\nAbu Dhabi is the capital of the United Arab Emirates.\n\nGeorgia is a North Caucasian country, is part of the\n### Actual Answer:\nSELECT Text/background colour FROM table WHERE State/territory = Australian Capital Territory"},"metadata":{}}]},{"cell_type":"markdown","source":"# Freeze layers","metadata":{}},{"cell_type":"code","source":"for params in model.parameters():\n    params.requires_grad = False # Freeze all parameter\n    if params.ndim == 1:\n        params.data = params.data.to(torch.float32) # cast to float32 for stability\n\n# Enables the gradients for the input embeddings.\n# This is useful for fine-tuning adapter weights while keeping the model weights fixed.\nmodel.enable_input_require_grads()\n# reduce number of stored activations\nmodel.gradient_checkpointing_enable()\nclass CastToFloat(nn.Sequential):\n    def forward(self,x):\n        return super().forward(x).to(torch.float32)\nmodel.lm_head = CastToFloat(model.lm_head)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:45:44.488136Z","iopub.execute_input":"2024-02-28T00:45:44.488997Z","iopub.status.idle":"2024-02-28T00:45:44.508970Z","shell.execute_reply.started":"2024-02-28T00:45:44.488960Z","shell.execute_reply":"2024-02-28T00:45:44.508182Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Lora Configuration","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\ntask = 'CAUSAL_LM'\ndesired_rank = 8\nlora_alpha = 32\nlora_dropout = 0.1\nlora_config = LoraConfig(\n    task_type = task,\n    r = desired_rank,\n    lora_alpha = lora_alpha,\n    lora_dropout = lora_dropout,\n    target_modules=target_modules,\n    bias = 'none',\n#     use_dora = True only for DoRA\n)\n\npeft_model = get_peft_model(model,lora_config)\npeft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:45:47.358514Z","iopub.execute_input":"2024-02-28T00:45:47.358884Z","iopub.status.idle":"2024-02-28T00:45:47.453425Z","shell.execute_reply.started":"2024-02-28T00:45:47.358854Z","shell.execute_reply":"2024-02-28T00:45:47.452358Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"trainable params: 1,843,200 || all params: 2,515,978,240 || trainable%: 0.07325977509249047\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:28:04.341055Z","iopub.execute_input":"2024-02-28T00:28:04.341431Z","iopub.status.idle":"2024-02-28T00:28:04.346743Z","shell.execute_reply.started":"2024-02-28T00:28:04.341403Z","shell.execute_reply":"2024-02-28T00:28:04.345661Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"GemmaTokenizerFast(name_or_path='/kaggle/input/gemma/transformers/2b/2', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# UnFreeze Layers","metadata":{}},{"cell_type":"code","source":"# for name,param in peft_model.named_parameters():\n#     if any(unfree in name for unfree in unfreeze):\n#             param.requires_grad_(False)\n\n            \n# trainable_params = {n:p for n,p in model.named_parameters() if p.requires_grad}","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:44:18.788295Z","iopub.execute_input":"2024-02-28T00:44:18.788999Z","iopub.status.idle":"2024-02-28T00:44:18.801616Z","shell.execute_reply.started":"2024-02-28T00:44:18.788959Z","shell.execute_reply":"2024-02-28T00:44:18.800501Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"peft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:45:57.288050Z","iopub.execute_input":"2024-02-28T00:45:57.288977Z","iopub.status.idle":"2024-02-28T00:45:57.299586Z","shell.execute_reply.started":"2024-02-28T00:45:57.288942Z","shell.execute_reply":"2024-02-28T00:45:57.298637Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"trainable params: 1,843,200 || all params: 2,515,978,240 || trainable%: 0.07325977509249047\n","output_type":"stream"}]},{"cell_type":"code","source":"# trainable_params.keys()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:28:19.974781Z","iopub.execute_input":"2024-02-28T00:28:19.975900Z","iopub.status.idle":"2024-02-28T00:28:19.980145Z","shell.execute_reply.started":"2024-02-28T00:28:19.975857Z","shell.execute_reply":"2024-02-28T00:28:19.979171Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"peft_model","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:46:03.198451Z","iopub.execute_input":"2024-02-28T00:46:03.199172Z","iopub.status.idle":"2024-02-28T00:46:03.215283Z","shell.execute_reply.started":"2024-02-28T00:46:03.199143Z","shell.execute_reply":"2024-02-28T00:46:03.214182Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): GemmaForCausalLM(\n      (model): GemmaModel(\n        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n        (layers): ModuleList(\n          (0-17): 18 x GemmaDecoderLayer(\n            (self_attn): GemmaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): GemmaRotaryEmbedding()\n            )\n            (mlp): GemmaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=16384, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=16384, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=16384, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): GELUActivation()\n            )\n            (input_layernorm): GemmaRMSNorm()\n            (post_attention_layernorm): GemmaRMSNorm()\n          )\n        )\n        (norm): GemmaRMSNorm()\n      )\n      (lm_head): CastToFloat(\n        (0): CastToFloat(\n          (0): Linear(in_features=2048, out_features=256000, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from torchdata.datapipes.map import SequenceWrapper, Mapper","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:46:12.232613Z","iopub.execute_input":"2024-02-28T00:46:12.232950Z","iopub.status.idle":"2024-02-28T00:46:12.332281Z","shell.execute_reply.started":"2024-02-28T00:46:12.232925Z","shell.execute_reply":"2024-02-28T00:46:12.331493Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# tamplate to adapt\n\ndef create_prompt(question, answer):\n    if len(answer) < 1:\n        answer = \"Cannot Find Answer\"\n\n    prompt_template = f\"### QUESTION\\n{question}\\n### ANSWER\\n{answer}</s>\"\n    return prompt_template\n\nmapped_qa_train = Mapper(train,lambda sample: tokenizer(create_prompt(sample['question'], sample['answer'])),\n                              )","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:46:12.336710Z","iopub.execute_input":"2024-02-28T00:46:12.336994Z","iopub.status.idle":"2024-02-28T00:46:12.342333Z","shell.execute_reply.started":"2024-02-28T00:46:12.336968Z","shell.execute_reply":"2024-02-28T00:46:12.341383Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"mapped_qa_val = Mapper(val,lambda sample: tokenizer(create_prompt(sample['question'], sample['answer'])))","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:46:12.796552Z","iopub.execute_input":"2024-02-28T00:46:12.796871Z","iopub.status.idle":"2024-02-28T00:46:12.801454Z","shell.execute_reply.started":"2024-02-28T00:46:12.796848Z","shell.execute_reply":"2024-02-28T00:46:12.800561Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning","metadata":{}},{"cell_type":"code","source":"epochs = 2\nfine_tune_tag = 'sql-qa-fine-tuned-model'\nmodel_name = 'gemma-2b'\ncontext_length = 8192\ngrad_accum = 2\nbatch_size = 4\nsave_dir = f'./results/{model_name}_{Dataset_id}_epochs_{epochs}_context_length_{context_length}'\nprint(save_dir)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:46:24.966006Z","iopub.execute_input":"2024-02-28T00:46:24.966391Z","iopub.status.idle":"2024-02-28T00:46:24.972357Z","shell.execute_reply.started":"2024-02-28T00:46:24.966359Z","shell.execute_reply":"2024-02-28T00:46:24.971393Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"./results/gemma-2b_wikisql_epochs_2_context_length_8192\n","output_type":"stream"}]},{"cell_type":"code","source":"# mapped_qa_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:28:32.232972Z","iopub.execute_input":"2024-02-28T00:28:32.233874Z","iopub.status.idle":"2024-02-28T00:28:32.237965Z","shell.execute_reply.started":"2024-02-28T00:28:32.233841Z","shell.execute_reply":"2024-02-28T00:28:32.236856Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer,TrainingArguments,DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2024-02-28T00:47:01.613531Z","iopub.execute_input":"2024-02-28T00:47:01.614150Z","iopub.status.idle":"2024-02-28T00:47:12.600518Z","shell.execute_reply.started":"2024-02-28T00:47:01.614113Z","shell.execute_reply":"2024-02-28T00:47:12.599490Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"2024-02-28 00:47:04.248758: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-28 00:47:04.248849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-28 00:47:04.374275: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer,TrainingArguments,DataCollatorForLanguageModeling\n# from trl import SFTTrainer\ntrainer = Trainer(\n    model = peft_model,\n#     tokenizer=tokenizer,\n    train_dataset= mapped_qa_train,\n    eval_dataset=mapped_qa_val,\n    args= TrainingArguments(\n#         save_steps = 20,\n        do_eval=True,\n\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size = batch_size,\n        gradient_accumulation_steps=grad_accum,\n        warmup_steps=100,\n        max_steps=20,\n        learning_rate=1e-3,\n        fp16=True,\n        logging_steps=1,\n        output_dir=save_dir,\n        log_level = 'debug',\n        lr_scheduler_type='constant',\n        max_grad_norm=0.3,\n#         evaluation_strategy='steps',\n#         eval_steps=0.2,\n    ),\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:14:19.688205Z","iopub.execute_input":"2024-02-28T01:14:19.689137Z","iopub.status.idle":"2024-02-28T01:14:19.715503Z","shell.execute_reply.started":"2024-02-28T01:14:19.689095Z","shell.execute_reply":"2024-02-28T01:14:19.714473Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nYou have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:14:21.620337Z","iopub.execute_input":"2024-02-28T01:14:21.621088Z","iopub.status.idle":"2024-02-28T01:17:53.037878Z","shell.execute_reply.started":"2024-02-28T01:14:21.621054Z","shell.execute_reply":"2024-02-28T01:17:53.036744Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 4\n***** Running training *****\n  Num examples = 56,355\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 2\n  Total optimization steps = 20\n  Number of trainable parameters = 1,843,200\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 03:20, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.737700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.239500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.077200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.262100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.246100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.327200</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.168200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.163100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.970400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.904800</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.051300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.964300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.450400</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.124400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.211300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.087100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.274500</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.100600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.281300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.317000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=1.1479340583086013, metrics={'train_runtime': 210.9718, 'train_samples_per_second': 0.758, 'train_steps_per_second': 0.095, 'total_flos': 115390565744640.0, 'train_loss': 1.1479340583086013, 'epoch': 0.0})"},"metadata":{}}]},{"cell_type":"code","source":"HUGGING_FACE_USER_NAME = \"ahmedelsayed\"\nmodel = 'gemma-2b'\npeft_model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model}\", use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:27:08.689379Z","iopub.execute_input":"2024-02-28T01:27:08.689750Z","iopub.status.idle":"2024-02-28T01:27:11.804989Z","shell.execute_reply.started":"2024-02-28T01:27:08.689719Z","shell.execute_reply":"2024-02-28T01:27:11.803903Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/gemma/transformers/2b/2 - will assume that the vocabulary was not modified.\n  warnings.warn(\nUploading the following files to ahmedelsayed/gemma-2b: adapter_config.json,README.md,adapter_model.safetensors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/39.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15b12d97d004c3eb851b76afd780c20"}},"metadata":{}},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ahmedelsayed/gemma-2b/commit/030b6c4070ac2b22c021cabc80e71fe09b6fb7d8', commit_message='Upload model', commit_description='', oid='030b6c4070ac2b22c021cabc80e71fe09b6fb7d8', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npeft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model}\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nqa_model = PeftModel.from_pretrained(model, peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:27:34.178877Z","iopub.execute_input":"2024-02-28T01:27:34.179543Z","iopub.status.idle":"2024-02-28T01:27:44.315580Z","shell.execute_reply.started":"2024-02-28T01:27:34.179508Z","shell.execute_reply":"2024-02-28T01:27:44.314474Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04dc839e6da6430eb7d9de0a829cab77"}},"metadata":{}},{"name":"stderr","text":"loading configuration file /kaggle/input/gemma/transformers/2b/2/config.json\nModel config GemmaConfig {\n  \"_name_or_path\": \"/kaggle/input/gemma/transformers/2b/2\",\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 18,\n  \"num_key_value_heads\": 1,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.38.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 256000\n}\n\nloading weights file /kaggle/input/gemma/transformers/2b/2/model.safetensors.index.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98cffc4d5e424ed68db8cdf62de1cc49"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing GemmaForCausalLM.\n\nAll the weights of GemmaForCausalLM were initialized from the model checkpoint at /kaggle/input/gemma/transformers/2b/2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\nloading configuration file /kaggle/input/gemma/transformers/2b/2/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nloading file tokenizer.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/39.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f843960c16bf42e49859d91dec236038"}},"metadata":{}}]},{"cell_type":"code","source":"def make_inference(question,qa_model):\n    batch = tokenizer(f\"### QUESTION\\n{question}\\n### ANSWER\\n\", return_tensors='pt')\n    qa_model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n    qa_model.eval()\n    with torch.cuda.amp.autocast():\n        output_tokens = qa_model.generate(**batch, max_new_tokens=200)\n\n    display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:27:59.617433Z","iopub.execute_input":"2024-02-28T01:27:59.617822Z","iopub.status.idle":"2024-02-28T01:27:59.625595Z","shell.execute_reply.started":"2024-02-28T01:27:59.617791Z","shell.execute_reply":"2024-02-28T01:27:59.624487Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"test[0]['question']","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:25:15.350802Z","iopub.execute_input":"2024-02-28T01:25:15.351171Z","iopub.status.idle":"2024-02-28T01:25:15.359516Z","shell.execute_reply.started":"2024-02-28T01:25:15.351131Z","shell.execute_reply":"2024-02-28T01:25:15.358398Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"\"What is terrence ross' nationality\""},"metadata":{}}]},{"cell_type":"code","source":"make_inference(test[0]['question'],qa_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:28:04.570655Z","iopub.execute_input":"2024-02-28T01:28:04.571024Z","iopub.status.idle":"2024-02-28T01:28:14.983153Z","shell.execute_reply.started":"2024-02-28T01:28:04.570994Z","shell.execute_reply":"2024-02-28T01:28:14.982116Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nWhat is terrence ross' nationality\n### ANSWER\nSELECT Nationality FROM table WHERE Player = Terrence Ross</s>"},"metadata":{}}]},{"cell_type":"code","source":"test[0]['answer']","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:28:32.215018Z","iopub.execute_input":"2024-02-28T01:28:32.215384Z","iopub.status.idle":"2024-02-28T01:28:32.223867Z","shell.execute_reply.started":"2024-02-28T01:28:32.215355Z","shell.execute_reply":"2024-02-28T01:28:32.222691Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"'SELECT Nationality FROM table WHERE Player = Terrence Ross'"},"metadata":{}}]},{"cell_type":"code","source":"for i, t in enumerate(test):\n    make_inference(t['question'],qa_model)\n    display(Markdown(f\"### Actual:\\n{t['answer']}\"))\n    if i==10:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-02-28T01:37:19.355772Z","iopub.execute_input":"2024-02-28T01:37:19.356516Z","iopub.status.idle":"2024-02-28T01:39:43.245269Z","shell.execute_reply.started":"2024-02-28T01:37:19.356480Z","shell.execute_reply":"2024-02-28T01:39:43.244261Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nWhat is terrence ross' nationality\n### ANSWER\nSELECT Nationality FROM table WHERE Player = Terrence Ross</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT Nationality FROM table WHERE Player = Terrence Ross"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nWhat clu was in toronto 1995-96\n### ANSWER\nSELECT Clu FROM table WHERE Season = toronto 1995-96</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT School/Club Team FROM table WHERE Years in Toronto = 1995-96"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nwhich club was in toronto 2003-06\n### ANSWER\nSELECT Club FROM table WHERE Year = toronto 2003-06</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT School/Club Team FROM table WHERE Years in Toronto = 2003-06"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nhow many schools or teams had jalen rose\n### ANSWER\nSELECT COUNT School/Team FROM table WHERE Player = Jalen Rose</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT COUNT School/Club Team FROM table WHERE Player = Jalen Rose"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nWhere was Assen held?\n### ANSWER\nSELECT Location FROM table WHERE Race = assen</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT Round FROM table WHERE Circuit = Assen"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nWhat was the number of race that Kevin Curtain won?\n### ANSWER\nSELECT Race FROM table WHERE Driver = kevin curtain</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT COUNT No FROM table WHERE Pole Position = Kevin Curtain"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nWhat was the date of the race in Misano?\n### ANSWER\nSELECT Date FROM table WHERE Location = misano</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT Date FROM table WHERE Circuit = Misano"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nHow many different positions did Sherbrooke Faucons (qmjhl) provide in the draft?\n### ANSWER\nSELECT COUNT Position FROM table WHERE Team = Sherbrooke Faucons (QMJHL)</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT COUNT Position FROM table WHERE College/junior/club team = Sherbrooke Faucons (QMJHL)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nWhat are the nationalities of the player picked from Thunder Bay Flyers (ushl)\n### ANSWER\nSELECT Nationality FROM table WHERE Team = thunder bay flyers (ushl)</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT Nationality FROM table WHERE College/junior/club team = Thunder Bay Flyers (USHL)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nHow many different college/junior/club teams provided a player to the Washington Capitals NHL Team?\n### ANSWER\nSELECT COUNT College/Junior/Club Teams FROM table WHERE Team = washington capitals</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT COUNT College/junior/club team FROM table WHERE NHL team = Washington Capitals"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### QUESTION\nHow many different nationalities do the players of New Jersey Devils come from?\n### ANSWER\nSELECT Nationality FROM table WHERE Team = new jersey devils</s>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Actual:\nSELECT COUNT Nationality FROM table WHERE NHL team = New Jersey Devils"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}